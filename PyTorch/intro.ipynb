{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M857YjYNvxvP"
      },
      "source": [
        "# Introduction to PyTorch\n",
        "\n",
        "Revision: 1.00 , Date: $16^{th}. May, 2021$\n",
        "\n",
        "Source: [erozita](https://github.com/ezorita/pytorch_notebooks/blob/master/1_introduction_to_pytorch.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "Let's start by importing all the essential modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_kB8Y4evlom"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNpicVTEwEjo"
      },
      "source": [
        "## Tensors and Gradients\n",
        "\n",
        "### 1. Tensors\n",
        "\n",
        "Tensors are like numpy arrays/matrices: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aUK0laTwQNy",
        "outputId": "93ebae1f-ff87-4b43-ca57-137c15796a59"
      },
      "source": [
        "a  = torch.tensor([[1, 2, 3], [1, 2, 3]], dtype=torch.float32)\n",
        "print(a)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [1., 2., 3.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6CBr6aGwgWh"
      },
      "source": [
        "There are several ways to initialize tensors in PyTorch, just like its the case with initializing numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZkysOYCwfYg",
        "outputId": "6e01407e-7c3d-42d3-f4d6-b0b5f45f4955"
      },
      "source": [
        "b = torch.ones(2,3)\n",
        "print(f'b: {b}')\n",
        "c = torch.zeros(2,3)\n",
        "print(f'c: {c}')\n",
        "d = torch.randn(4, 4)\n",
        "print(f'd: {d}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b: tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "c: tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "d: tensor([[ 1.5727, -0.9121, -1.2754,  0.8197],\n",
            "        [-0.3737, -0.4267,  0.7139,  0.1408],\n",
            "        [ 0.2777,  0.2037,  0.7357, -0.1995],\n",
            "        [ 1.1090, -0.3205,  0.4893, -0.3355]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuGrpe23xiX3"
      },
      "source": [
        "Numpy arrays can be converted to tensors using `from_numpy` functions or im"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03bDByFPwfV3",
        "outputId": "fd19ef05-72e2-4189-c6b1-0e9eb20e6d3d"
      },
      "source": [
        "a = np.array([[-3, -2, -1], [97, 98, 99]])\n",
        "t1 = torch.from_numpy(a)\n",
        "print(f't1: {t1}')\n",
        "\n",
        "t2 = torch.tensor(a)\n",
        "print(f't2: {t2}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t1: tensor([[-3, -2, -1],\n",
            "        [97, 98, 99]])\n",
            "t2: tensor([[-3, -2, -1],\n",
            "        [97, 98, 99]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FizDvFCzPev"
      },
      "source": [
        "we can simply do element wise arithemetic operations with tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHgnDk4AwfO4",
        "outputId": "a8e2d4ba-930e-47b7-953f-667bb4df18cc"
      },
      "source": [
        "a = torch.tensor([[1,0],[0,1]])\n",
        "b = torch.tensor([[1,-1],[-1,1]])\n",
        "print(f'a+b: {a+b}')\n",
        "print(f'a-b: {a-b}')\n",
        "print(f'a*b: {a*b}')\n",
        "print(f'a/b: {a/b}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a+b: tensor([[ 2, -1],\n",
            "        [-1,  2]])\n",
            "a-b: tensor([[0, 1],\n",
            "        [1, 0]])\n",
            "a*b: tensor([[1, 0],\n",
            "        [0, 1]])\n",
            "a/b: tensor([[1., -0.],\n",
            "        [-0., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbRMD9Y7zwvR"
      },
      "source": [
        "we can also perform other operations like matrix multiplications using `torch.mm`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7jxwW7H0Hot",
        "outputId": "27f158b8-f387-43a8-d9b2-cd81d51073e1"
      },
      "source": [
        "c = torch.mm(a, b) # I(identity-matrix) X B = B\n",
        "print(f'c: {c}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "c: tensor([[ 1, -1],\n",
            "        [-1,  1]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt0VaJU30f9R"
      },
      "source": [
        "### 2. Tensor Gradients\n",
        "\n",
        "By default PyTorch tensors represent function constants. We can convert them to function variables (weights) by setting `required_grad` (boolean, argument of a tensor constructor) **True**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiLPBXlY0HsT",
        "outputId": "eaca8970-0be7-463c-c3d2-b6346465be6a"
      },
      "source": [
        "t = torch.tensor([[1,1],[2,2]], dtype=torch.float32)\n",
        "print(f'default required_grad value of t: {t.requires_grad}')\n",
        "\n",
        "t = torch.tensor([1,2,3,4], dtype=torch.float32, requires_grad=True)\n",
        "print(f\"once its initialized as a weight matrix t's requires_grad: {t.requires_grad}\")\n",
        "\n",
        "# We can set this to false or true later as well to useful to freeze some layers\n",
        "t.requires_grad = False\n",
        "print(f\"resetting it to constant matrix t's requires_grad: {t.requires_grad}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "default required_grad value of t: False\n",
            "once its initialized as a weight matrix t's requires_grad: True\n",
            "resetting it to constant matrix t's requires_grad: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qih7Dbd32oyK"
      },
      "source": [
        "## Optimization\n",
        "\n",
        "\n",
        "### Fitting a linear model\n",
        "Model description\n",
        "\n",
        "As an example, we want to optimize the fitting of a linear model to several datapoints using tensor gradients. We will use stochastic gradient descent to find the values of our parameters that minimize the error between the modeled line and the datapoints.\n",
        "\n",
        "Since we are fitting a linear model, the parameters to optimize are the slope $a$ and the offset $b$ of the linear equation\n",
        "$$\\hat{y}(x) = a x + b$$\n",
        "\n",
        "Optimizing the model reduces to finding the values of $a$ and $b$ that minimize the mean squared error of the data samples $(x_i,y_i)$:\n",
        "$$\\min_{a,b} \\frac{1}{N}\\sum_{i=1}^N (y_i-\\hat{y}(x_i))^2$$\n",
        "Data samples\n",
        "\n",
        "We start by defining our data samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8GKWrGp0gUq"
      },
      "source": [
        "x = torch.tensor([0,1,2,3,4,5,6,7,8, 10], dtype=torch.float32)\n",
        "a = torch.ones(10)\n",
        "a = a*5\n",
        "# adding some randomness\n",
        "randomness = torch.rand(10)\n",
        "\n",
        "y = a*x + randomness"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZ0CwKLiB2JR"
      },
      "source": [
        "We can easily visualize the datapoints in a 2D tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5d77cSR0g8O",
        "outputId": "0493d19a-14f5-4847-f2af-f66541d94d9a"
      },
      "source": [
        "xy = torch.stack((x,y))\n",
        "print(torch.t(xy))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.6866],\n",
            "        [ 1.0000,  5.4072],\n",
            "        [ 2.0000, 10.5659],\n",
            "        [ 3.0000, 15.4073],\n",
            "        [ 4.0000, 20.3650],\n",
            "        [ 5.0000, 25.4268],\n",
            "        [ 6.0000, 30.4522],\n",
            "        [ 7.0000, 35.6960],\n",
            "        [ 8.0000, 40.5056],\n",
            "        [10.0000, 50.3924]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4l0jiQBCBuT"
      },
      "source": [
        "\n",
        "Model parameters\n",
        "\n",
        "We allocate two tensors a and b as parameters (weight metrices)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58aGnIs30g1e",
        "outputId": "9a955b7a-6267-47a9-e777-29d8a6a50511"
      },
      "source": [
        "a = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "b = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "print(f'a: {a}')\n",
        "print(f'b: {b}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a: 0.0\n",
            "b: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvHJY_twCVOy"
      },
      "source": [
        "\n",
        "Cost function\n",
        "\n",
        "Also known as the loss function, is the target function to optimize, in our case the mean squared error and lets see the loss with initial parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9LjtFxD0gyz",
        "outputId": "5c612a5c-771d-419e-a135-3091c5d81431"
      },
      "source": [
        "loss = 1/len(x)*torch.sum(torch.pow(y - a*x - b, 2))\n",
        "print(loss)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(782.1626, grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4ppx_2vClDh"
      },
      "source": [
        "\n",
        "### The optimizer\n",
        "\n",
        "#### Gradient Updates\n",
        "\n",
        "The optimizer is a buffer that collects the computed gradients and updates the parameters according to an algorithm. The update is triggered every time we call the method `step()`. It is important to remark that the **optimizer does not compute the gradients**.\n",
        "\n",
        "#### Gradient Computation\n",
        "\n",
        "The gradients are computed and stored in the tensors whose `require_grad` attribute is set to **True**. To compute the gradients we have to use `backward()` on a Tensor that represents a function of the parameters.\n",
        "\n",
        "#### Gradient Update Algorithm\n",
        "\n",
        "We will be using Stochastic Gradient Descent for optimization. The algorithm is defined in `torch.optim.SGD` and requires the parameters to be optimized and the learning rate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR_Ilp0rCmZS"
      },
      "source": [
        "optimizer = torch.optim.SGD([a,b], lr=0.01)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwytQcf2Ck8-"
      },
      "source": [
        "\n",
        "\n",
        "It is important to reset the gradients stored in the optimizer before each gradient computation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7tmEQrs0gvg",
        "outputId": "7e2ff02c-1c3a-4b43-aba9-d50b16d5fb13"
      },
      "source": [
        "optimizer.zero_grad()\n",
        "\n",
        "# the gradients are now set to 0\n",
        "print(a.grad)\n",
        "print(b.grad)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blaWin4PDSwO"
      },
      "source": [
        "We want to compute the gradients of the parameters wrt the Loss function. To do so, we use the `backward()` on Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rhnk_5BtDKsJ",
        "outputId": "16879f91-731a-4eb1-ace5-e8d92d97115b"
      },
      "source": [
        "loss.backward()\n",
        "\n",
        "# check the gradients\n",
        "print(a.grad)\n",
        "print(b.grad)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-616.7635)\n",
            "tensor(-93.9620)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ihe-g7uDy7P"
      },
      "source": [
        "Once the gradients are computed, we perform a gradient descent step and reset the stored gradients\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfDmzuYoDKmX"
      },
      "source": [
        "optimizer.step()\n",
        "optimizer.zero_grad()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAKOa0ikEGqy"
      },
      "source": [
        "Let's check how the values of $a$ and $b$ have changed and whether the loss has decreased"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZIqjOAvDKfB",
        "outputId": "71566742-e675-4234-e30e-d7e88f15f115"
      },
      "source": [
        "print(f'new value of a: {a}')\n",
        "print(f'new value of b: {b}')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new value of a: 6.167635440826416\n",
            "new value of b: 0.9396199584007263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgkYAH4wELQ7",
        "outputId": "50109dfe-e9f5-413c-e8c3-f57892fe82aa"
      },
      "source": [
        "loss = 1/len(x)*torch.sum(torch.pow(y - a*x - b, 2))\n",
        "print(loss)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(46.6387, grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8wnGXEzEQIz"
      },
      "source": [
        "\n",
        "## SGD iterations\n",
        "\n",
        "Each SGD step updates our parameters according to the current gradients and the learning rate. We need several iterations to find the optimal values for $a$ and $b$. Let's iterate the same procedure 10 times to refine the fit of the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkQpM6SPEQhD",
        "outputId": "5d2aa6d3-9c65-4b0e-d56b-a4134074235d"
      },
      "source": [
        "for i in np.arange(10):\n",
        "    # Reset gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Compute loss and gradients\n",
        "    loss = 1/len(x)*torch.sum(torch.pow(y - a*x - b, 2))\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Print loss of previous step\n",
        "    print(f'Loss: {loss}')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(46.6387, grad_fn=<MulBackward0>)\n",
            "tensor(6.6865, grad_fn=<MulBackward0>)\n",
            "tensor(0.9796, grad_fn=<MulBackward0>)\n",
            "tensor(0.1643, grad_fn=<MulBackward0>)\n",
            "tensor(0.0476, grad_fn=<MulBackward0>)\n",
            "tensor(0.0308, grad_fn=<MulBackward0>)\n",
            "tensor(0.0283, grad_fn=<MulBackward0>)\n",
            "tensor(0.0277, grad_fn=<MulBackward0>)\n",
            "tensor(0.0275, grad_fn=<MulBackward0>)\n",
            "tensor(0.0273, grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke08oCqIEfS9"
      },
      "source": [
        "After 10 steps we see that the loss has converged. We can try to improve it by doing smaller steps (reducing the learning rate), but it's important to remember that the Loss will never reach 0 because the data does not perfectly fit a linear distribution.\n",
        "\n",
        "Let's iterate again with **LR=0.001**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFuql5XKEbTN"
      },
      "source": [
        "optimizer.param_groups[0]['lr'] = 1e-3"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3EMw0uoEmCF",
        "outputId": "0da25eb9-d84a-416b-a8be-b83096b2c6d6"
      },
      "source": [
        "for i in np.arange(10):\n",
        "    # Reset gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Compute loss and gradients\n",
        "    loss = 1/len(x)*torch.sum(torch.pow(y-a*x-b, 2))\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Print loss of previous step\n",
        "    print(f'Loss: {loss}')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0272, grad_fn=<MulBackward0>)\n",
            "tensor(0.0271, grad_fn=<MulBackward0>)\n",
            "tensor(0.0271, grad_fn=<MulBackward0>)\n",
            "tensor(0.0271, grad_fn=<MulBackward0>)\n",
            "tensor(0.0271, grad_fn=<MulBackward0>)\n",
            "tensor(0.0271, grad_fn=<MulBackward0>)\n",
            "tensor(0.0270, grad_fn=<MulBackward0>)\n",
            "tensor(0.0270, grad_fn=<MulBackward0>)\n",
            "tensor(0.0270, grad_fn=<MulBackward0>)\n",
            "tensor(0.0270, grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceMhxitoEylb"
      },
      "source": [
        "Indeed, it seems that the Loss has converged.\n",
        "## Results\n",
        "\n",
        "The final values after optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwMdV13nEzJU",
        "outputId": "86c08e79-f5bf-4750-a451-1944f4a27de1"
      },
      "source": [
        "print(f'Loss: {loss}')\n",
        "print(f'a: {a}')\n",
        "print(f'b: {b}')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.02699713408946991\n",
            "a: 4.959807395935059\n",
            "b: 0.7400157451629639\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUwDIz0eFEPg"
      },
      "source": [
        "And this is how our linear model looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "7nR8V0SsFEuX",
        "outputId": "eea694da-01c6-4d87-c96a-9644071d7c05"
      },
      "source": [
        "plt.scatter(x.numpy(), y.numpy())\n",
        "plt.plot(np.arange(11), np.arange(11)*a.detach().numpy()+b.detach().numpy(), 'k')\n",
        "plt.grid()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}